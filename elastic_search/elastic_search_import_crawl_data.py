#!/usr/bin/python

"""

This file takes as input the file generated by the web_crawl processing chain that contains a "list of all crawled urls"
(these URLs having as id their position in this list), the patht to the web "repository" that is to say where we had 
downloaded and stored the content of the web pages that we crawled, and the elasticsearch host.

It will go, in order, to through all the URLs in the list, load their content from disk, parse the HTML, extract the 
title and the description of each page, strip of the styling, comments and javascript. Keep only the "textual" version
of the document (without any HTML tags) and index this into elasticsearch.

It commits every batch_size (input argument, default value 1000) and thus should not use too much RAM if you do not set
a too high batch size (but a too small one will likely slow down insertion).

But this will likely take a lot of time, ranging from 5 to 16 hours. Go grab some popcorn.

"""


from json import loads
import sys
from time import time
from hashlib import sha1
from gzip import open as gzopen
from os.path import join as pjoin
import chardet
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk

DBG = False
index_name = "web_crawl_bm25_with_desc_final"  #"web_crawl_bm25_with_desc"
doc_type = "web_page"

def decode(s, encodings=('ascii', 'utf8', 'latin1', 'latin9')):
    for encoding in encodings:
        try:
            return s.decode(encoding)
        except UnicodeDecodeError:
            pass
    return None

def univ_encode(s):
    try:
        snew = decode(s)
        if snew is not None:
            s = snew
        else:
            result = chardet.detect(s)
            charenc = result['encoding']
            s = unicode(s, charenc, errors='ignore')
        return s.encode('utf8')
    except UnicodeDecodeError:
        return unicode(s, 'utf8', errors='ignore').encode('utf8')


def init_index(client, index, doc_type):
    client.indices.put_mapping(
        index=index,
        doc_type='web_page',
        body={
          doc_type: {
            "_all" : {"enabled" : "false"},
            "_source" : {"enabled" : "false"},
            "_id": {"store": "true"},
            'properties': {        
                'url': {
                  'type': 'string',
                  "store": "true",
                  'index': 'not_analyzed'
                },
                'title': {
                  'type': 'string',
                  'similarity': 'BM25',
                  "store": "true"
                },
                'content': {
                  'type': 'string',
                  'similarity': 'BM25',
                  "store": "true"
                },
                'description': {
                  'type': 'string',
                  'similarity': 'BM25',
                  "store": "true"
                }
            }
          }
        }
    )


def commit_away(client, array, index_name, doc_type):
    return bulk(
        client,
        array,
        index=index_name,
        doc_type=doc_type
    )

CLI_ARGS = ["ids_to_urls_from_web_crawl.lst.json", "web_crawl_page_contents_folder_path", "elasticsearch_host"]
OPTIONAL_PARAMS = ["batch_size=1000", "no_html=true", "offset_start=0", "limit_end_id=no_limit"]
def main():
    t0 = time()

    if len(sys.argv) < (len(CLI_ARGS)+1):
        print "Usage:", sys.argv[0], " ".join(CLI_ARGS), " ".join("[%s]" % _ for _ in OPTIONAL_PARAMS)
        exit()

    fname = sys.argv[1]
    web_crawl_page_contents_folder_path = sys.argv[2]
    host = sys.argv[3]
    
    batch_size = 500  # The batch size is how many rows we commit at once to elasticsearch
    if len(sys.argv) > (len(CLI_ARGS)+1):
        try:
            batch_size = int(sys.argv[len(CLI_ARGS)+1])
        except ValueError:
            pass

    print "Batch size=", batch_size

    if len(sys.argv) > (len(CLI_ARGS)+2):
        no_html = True if sys.argv[len(CLI_ARGS)+2].strip().lower() is not "no" else False
    else:
        no_html = True

    offset_start = 0
    if len(sys.argv) > (len(CLI_ARGS)+3):
        try:
            val = int(sys.argv[len(CLI_ARGS)+3].strip())
            offset_start = 0 if val < 0 else val
        except ValueError:
            print "No offset_start"
            pass

    limit_end = None
    if len(sys.argv) > (len(CLI_ARGS)+4):
        try:
            val = int(sys.argv[len(CLI_ARGS)+4].strip())
            limit_end = None if val < 0 or val < offset_start else val
        except ValueError:
            print "No limit_end passed"
            pass

    print "Working from index", offset_start, "until limit index", limit_end

    # If we are asked to strip off the HTML, we'll need BeautifulSoup
    if no_html is True:
        from bs4 import BeautifulSoup
        import bs4

    # Iterate through the list of all the URL we crawled, they are in the order of their ids
    # Note that as we are processing things as soon as we read them (though, batched)
    # we should not have memory issues related to the fact that fname file is huge (>1G)
    with gzopen(fname, 'r') as f:
        print "File opened"
        # Initialize API Client & Index
        client = Elasticsearch(host=host, port=9200)
        print "Connected to ES"
        init_index(client, index_name, doc_type)

        # Init vars for the big loop
        array = []
        i = -1
        n = 0
        t1 = time()

        # Big loop!
        for l in f:
            i += 1

            if i < offset_start:  # skip lines until we reach our starting offset
                continue

            if limit_end is not None and i >= limit_end:  # when we reached the maximum line to be processed, we break
                break

            url = l.strip()
            hashstr = sha1(url).hexdigest()
            contents_fname = pjoin(web_crawl_page_contents_folder_path, hashstr + ".html.gz")
            try:
                with gzopen(contents_fname, 'rb') as fcontents:
                    contents = fcontents.read()  # this reads the entire file
                    if no_html is True:
                        # Note: BeautifulSoup is supposed to handle the encoding automagically
                        b = BeautifulSoup(contents)
                        for elem in b.findAll(['script', 'style']):
                            elem.extract()
                        # print b.html.contents

                        try:
                            description = b.findAll(attrs={"name":"description"})[0].attrs['content']
                        except Exception:
                            # Whatever went wrong, we somehow did not find the description content, set it empty
                            description = ''

                        title = b.title.get_text() if b.title is not None \
                                else b.h1.get_text() if b.h1 is not None \
                                else 'Untitled'
                        '''
                        Test case of the previous conditions, (to be used in case of neede debugging)
                            In [37]: b = BeautifulSoup("<html><head></head><body><borg></borg><div><h1>Me? <strong>really?</strong></h1></div></html>")

                            In [38]: b.title.get_text() if b.title is not None else b.h1.get_text() if b.h1 is not None else 'Untitled'
                            Out[38]: u'Me? really?'
                        '''

                        contents = ' '.join([\
                            _.get_text() if isinstance(_, bs4.element.Tag) \
                            else unicode(_) \
                            for _ in b.html.contents \
                            if not isinstance(_, bs4.element.Comment) \
                        ]) if b.html is not None else b.get_text()
                    else:
                        contents = univ_encode(contents)
                        title = ''
                        description = ''
            except IOError as err:
                if DBG:
                    sys.stderr.write(str(err))
                    sys.stderr.write("\nUnable to read the content of the URL %s (hash=%s, file=%s)" % (url, hashstr, contents_fname))
                    sys.stderr.write("\nIt was likely not crawled, skipping it.\n")
                continue
            row = {
                "title": title,
                "url": url,
                "hash": hashstr,
                "content": contents,
                "description": description,
                "_id": i  # The id is nothing else than the index in the list that we are currently iterating
            }
            # Note: The ES "bulk" format is: one row defining the action of the next row, then the action data on next row
            array.append(row)
            n += 1
            if len(array) == batch_size:
                print "Committing a batch of", batch_size, "rows."
                # Commit a batch away!
                try:
                    commit_away(client, array, index_name, doc_type)
                except Exception as e:
                    # Something went wrong, wait for user input before continuing, but we do not want
                    # to cancel the entire thing (and thus to have to run it again...)
                    # just because some stupid unexpected exception happened
                    print "Something went wrong:", e
                    ans = raw_input("Continue anyway? [Y/n]")
                    if ans == "no" or ans == "n":
                        print "Aborting..."
                        break
                array = []

                print "Rows committed, committed", n, "rows so far (", n/(time()-t1), "rows per second)"
                sys.stdout.flush()
        
        # end for l in f <-- yeah, we should just define a function and not have such a big loop, TODO!
        
        # Commit the remaining in the last batch
        if array:
            commit_away(client, array, index_name, doc_type)

    # refresh to make the documents available for search
    client.indices.refresh(index=index_name)

    # and now we can count the documents
    print(client.count(index=index_name)['count'], 'documents in index')
    print "Done in", time()-t0


if __name__ == '__main__':
    main()

